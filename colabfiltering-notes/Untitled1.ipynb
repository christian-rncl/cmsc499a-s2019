{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, k=30):\n",
    "        super().__init__()\n",
    "        self.U = nn.Embedding(n_users, k)\n",
    "        self.M = nn.Embedding(n_movies, k)\n",
    "        #initialize embedding matrices\n",
    "        torch.nn.init.kaiming_normal_(self.U.weight.data)\n",
    "        torch.nn.init.kaiming_normal_(self.M.weight.data)\n",
    "    \n",
    "    def forward(self, usr_idx, mov_idx):\n",
    "        return (self.U(usr_idx) * self.M(mov_idx)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(ni,nf):\n",
    "    e = nn.Embedding(ni, nf)\n",
    "    e.weight.data.uniform_(-0.01,0.01)\n",
    "    return e\n",
    "\n",
    "class EmbeddingDotBias(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors):\n",
    "        super().__init__()\n",
    "        (self.u, self.m, self.ub, self.mb) = [get_emb(*o) for o in [\n",
    "            (n_users, n_factors), (n_movies, n_factors), (n_users,1), (n_movies,1)\n",
    "        ]]\n",
    "        \n",
    "    def forward(self, users, movies):\n",
    "        um = (self.u(users)* self.m(movies)).sum(1)\n",
    "        res = um + self.ub(users).squeeze() + self.mb(movies).squeeze()\n",
    "#        res = torch.sigmoid(res) * (9-1) + 1\n",
    "        return res.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([\n",
    "    [4, 2, -1, -1],\n",
    "    [6, 9, -1, 1],\n",
    "    [1, -1, 3, -1],\n",
    "    [5, -1, -1, 4],\n",
    "    [-1, 3, 4, 5],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatrixFactorization(test.shape[0], test.shape[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingDotBias(test.shape[0], test.shape[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.argwhere(test > -1)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, j in Z:\n",
    "      #need to use variables for autograd / loss function\n",
    "        m_ij = Variable(torch.FloatTensor([test[i,j]]))\n",
    "        i = Variable(torch.LongTensor([i]))\n",
    "        j = Variable(torch.LongTensor([j]))\n",
    "\n",
    "        m_ij_hat = model(i,j)\n",
    "        loss = l(m_ij, m_ij_hat)\n",
    "\n",
    "        #backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1652.2900]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Variable(torch.LongTensor([0])),Variable(torch.LongTensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1652.2900]) tensor([-25.7033]) tensor([54.2778]) tensor([2.0120]) \n",
      "tensor([-1197.5562]) tensor([-0.8996]) tensor([49.6696]) tensor([-13.8453]) \n",
      "tensor([1791.9508]) tensor([5.2046]) tensor([-67.6159]) tensor([-9.6897]) \n",
      "tensor([-16872.3398]) tensor([150.5946]) tensor([663.4835]) tensor([-239.5280]) \n",
      "tensor([-264.5982]) tensor([-0.7314]) tensor([1.5184]) tensor([7.4993]) \n"
     ]
    }
   ],
   "source": [
    "for i in range(test.shape[0]):\n",
    "    for j in range(test.shape[1]):\n",
    "        print(str(model(Variable(torch.LongTensor([i])),Variable(torch.LongTensor([j]))).data[0]) + ' ', end='')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import rand as sprand\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Make up some random explicit feedback ratings\n",
    "# and convert to a numpy array\n",
    "n_users = 10\n",
    "n_items = 10\n",
    "ratings = np.random.rand(n_users, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= EmbeddingDotBias(ratings.shape[0], ratings.shape[1], 100)\n",
    "l = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)#, weight_decay=1e-2)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort our data\n",
    "rows, cols = ratings.nonzero()\n",
    "p = np.random.permutation(len(rows))\n",
    "rows, cols = rows[p], cols[p]\n",
    "epochs = 30\n",
    "\n",
    "for e in range(epochs):\n",
    "    for row, col in zip(*(rows, cols)):\n",
    "        # Turn data into variables\n",
    "        rating = Variable(torch.FloatTensor([ratings[row, col]]))\n",
    "        row = Variable(torch.LongTensor([np.long(row)]))\n",
    "        col = Variable(torch.LongTensor([np.long(col)]))\n",
    "\n",
    "        # Predict and calculate loss\n",
    "        prediction = model(row, col)\n",
    "        loss = l(prediction, rating)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39623922 0.85700251 0.02117972 0.05625527 0.83430948]\n",
      "[0.44202626 0.27656302 0.37642793 0.42788834 0.46807313]\n",
      "[0.98306338 0.9753429  0.80496421 0.87571331 0.29794588]\n",
      "[0.64034244 0.59277837 0.16859842 0.79248708 0.75846907]\n",
      "[0.16666699 0.03781352 0.69918887 0.78551424 0.29208783]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(ratings[i,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.2283]) tensor([23.6450]) tensor([2.4460]) tensor([1.0319]) tensor([8.1186]) \n",
      "tensor([-0.1754]) tensor([-9.6513]) tensor([-0.3928]) tensor([1.4186]) tensor([21.7434]) \n",
      "tensor([5.2027]) tensor([-12.1442]) tensor([2.7429]) tensor([-2.3188]) tensor([1.1454]) \n",
      "tensor([-6.4635]) tensor([8.9304]) tensor([1.6601]) tensor([-5.0688]) tensor([-1.9566]) \n",
      "tensor([-2.7202]) tensor([15.9583]) tensor([1.3614]) tensor([-1.0350]) tensor([-5.5708]) \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    row = Variable(torch.LongTensor([np.long(i)]))\n",
    "    for j in range(5):\n",
    "        col = Variable(torch.LongTensor([np.long(j)]))\n",
    "        print(str(model(row, col).data[0]) + \" \", end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
