{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "bs = 64\n",
    "test_pct = .10\n",
    "val_pct = .60\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "from make_matrix import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, Sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fludf = pd.read_csv('flu_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>virusUprot</th>\n",
       "      <th>humanUprot</th>\n",
       "      <th>edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>P03433</td>\n",
       "      <td>P49736</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P03433</td>\n",
       "      <td>P15311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>P03433</td>\n",
       "      <td>P11142</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P03433</td>\n",
       "      <td>Q86U42</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>P03433</td>\n",
       "      <td>P33992</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 virusUprot humanUprot  edge\n",
       "0           0     P03433     P49736   1.0\n",
       "1           1     P03433     P15311   0.0\n",
       "2           2     P03433     P11142   0.0\n",
       "3           3     P03433     Q86U42   0.0\n",
       "4           4     P03433     P33992   1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fludf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "from Bio.Alphabet import ProteinAlphabet\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pickle.load(open('D.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dict.fromkeys(['A', 'G','V'], 1)\n",
    "classes.update(dict.fromkeys(['I', 'L', 'F', 'P'], 2))\n",
    "classes.update(dict.fromkeys(['Y', 'M', 'T', 'S'], 3))\n",
    "classes.update(dict.fromkeys(['H', 'N', 'Q', 'W'], 4))\n",
    "classes.update(dict.fromkeys(['R', 'K'], 5))\n",
    "classes.update(dict.fromkeys(['D', 'E'], 6))\n",
    "classes.update(dict.fromkeys(['C', 'U'], 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protToClass(p):\n",
    "    if p == 'X':\n",
    "        return '-1'\n",
    "    else:\n",
    "        return str(classes[p])\n",
    "\n",
    "def seqToClass(seq):\n",
    "    return ''.join(list(map(protToClass, seq)))\n",
    "\n",
    "def normalize(Fi):\n",
    "    return (Fi - min(Fi)) / max(Fi)\n",
    "\n",
    "def getFi(D, seq):\n",
    "    grptoi = {p:i for i,p in enumerate(D)} # group to index mappings\n",
    "    Fi = np.zeros(len(grptoi.values()))\n",
    "    \n",
    "    classSeq = seqToClass(seq)\n",
    "#     print(classSeq)\n",
    "    \n",
    "    for p in D:\n",
    "        Fi[grptoi[p]] += classSeq.count(''.join(p))\n",
    "\n",
    "    return normalize(Fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proteinize(records):\n",
    "    for r in records:\n",
    "        r.seq.Alphabet = ProteinAlphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_records = list(SeqIO.parse('virus_prots.fasta', 'fasta'))\n",
    "human_records = list(SeqIO.parse('human_prots.fasta', 'fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinize(virus_records)\n",
    "proteinize(human_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q9WMX2\n"
     ]
    }
   ],
   "source": [
    "print(virus_records[0].name.split('|')[1]) # to get names of viruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProt(fasta):\n",
    "    return fasta.name.split('|')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary with k:v gene:Fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatMap(records):\n",
    "    sidefeats_map = {}\n",
    "    \n",
    "    for fasta in records:\n",
    "        prot = getProt(fasta)\n",
    "        sidefeats_map[prot] = getFi(D, fasta.seq)\n",
    "        \n",
    "    return sidefeats_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_featmap = getFeatMap(virus_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_featmap = getFeatMap(human_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalize proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(df):\n",
    "    vs = df['virusUprot'].unique()\n",
    "    hs = df['humanUprot'].unique()\n",
    "    vs_map = {v:i for i,v in enumerate(vs)}\n",
    "    hs_map = {h:i for i,h in enumerate(hs)}\n",
    "    \n",
    "    return vs_map, hs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_map, hs_map = numericalize(fludf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatMatrix(prots, ptoi, featmap):\n",
    "    feats = np.zeros((len(prots), 2793))\n",
    "    \n",
    "    for p in prots:\n",
    "#         print(p)\n",
    "        feats[ptoi[p], :] =  featmap[p]\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfeats = createFeatMatrix(fludf['virusUprot'].unique(), vs_map, v_featmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfeats = createFeatMatrix(fludf['humanUprot'].unique(), hs_map, h_featmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_to_torch(arr, dtype):\n",
    "    t =  torch.from_numpy(np.array(arr)).type(dtype).to(device)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoaders(df):\n",
    "    X = list(zip(df.virusUprot.values, df.humanUprot.values))\n",
    "    y = df['edge'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_pct, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_pct, random_state=1)\n",
    "    \n",
    "    train_dset = TensorDataset(arr_to_torch(X_train, torch.long), arr_to_torch(y_train, torch.float32))\n",
    "    train_loader = DataLoader(train_dset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    val_dset = TensorDataset(arr_to_torch(X_val, torch.long), arr_to_torch(y_val, torch.float32))\n",
    "    val_loader = DataLoader(val_dset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    test_dset = TensorDataset(arr_to_torch(X_test, torch.long), arr_to_torch(y_test, torch.float32))\n",
    "    test_loader = DataLoader(test_dset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fludf.virusUprot = fludf.virusUprot.apply(lambda x : vs_map[x])\n",
    "fludf.humanUprot = fludf.humanUprot.apply(lambda x : hs_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "flutrain_loader, fluval_loader, flutest_loader = getLoaders(fludf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMF(nn.Module):\n",
    "    def __init__(self, n_virus, n_human, vfeats, hfeats, c_vector=1.0, c_bias=1.0, writer=None):\n",
    "        super(BMF, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = vfeats.shape[1]\n",
    "        self.n_virus = n_virus\n",
    "        self.n_human = n_human\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.virus = nn.Embedding(n_virus, 2793)\n",
    "        self.human = nn.Embedding(n_human, 2793)\n",
    "#         self.virus = nn.Parameter(n_virus, 2793)\n",
    "#         self.human = nn.Parameter(n_human, 2793)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_virus = nn.Embedding(n_virus, 1)\n",
    "        self.bias_human = nn.Embedding(n_human, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        self.vfeats = nn.Embedding(n_virus, 2793)\n",
    "        self.vfeats.weight = nn.Parameter(torch.from_numpy(vfeats).to(device))\n",
    "        self.vfeats.weight.requires_grad = False\n",
    "        \n",
    "        self.hfeats = nn.Embedding(n_human, 2793)\n",
    "        self.hfeats.weight = nn.Parameter(torch.from_numpy(hfeats).to(device))\n",
    "        self.hfeats.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        virus_id = train_x[:, 0]\n",
    "        human_id = train_x[:, 1]\n",
    "        \n",
    "        virus_feats = self.virus(virus_id)\n",
    "        human_feats = self.human(human_id)\n",
    "        vector_virus = self.virus(virus_id)\n",
    "        vector_human = self.human(human_id)\n",
    "        \n",
    "#         print('feats: ', virus_feats.shape, human_feats.shape)\n",
    "#         print('vecs:', vector_virus.shape, vector_human.shape)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_virus = self.bias_virus(virus_id).squeeze()\n",
    "        bias_human = self.bias_human(human_id).squeeze()\n",
    "        biases = (self.bias + bias_virus + bias_human)\n",
    "        \n",
    "        xU = torch.sum(virus_feats * vector_virus, dim=1)\n",
    "        xUV = torch.sum(xU * torch.t(vector_human), dim=1)\n",
    "        xUVy = torch.sum(xUV * human_feats, dim=1)\n",
    "\n",
    "        prediction = xUVy + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "#         loss_mse = F.binary_cross_entropy_with_logits(prediction, target.squeeze())\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "    \n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_virus =  l2_regularize(self.bias_virus.weight) * self.c_bias\n",
    "        prior_bias_human = l2_regularize(self.bias_human.weight) * self.c_bias\n",
    "        \n",
    "        prior_virus =  l2_regularize(self.virus.weight.data) * self.c_vector\n",
    "        prior_human = l2_regularize(self.human.weight.data) * self.c_vector\n",
    "        total = loss_mse + prior_virus + prior_human + prior_bias_virus + prior_bias_human\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss, Accuracy, Precision, Recall\n",
    "from tensorboardX import SummaryWriter\n",
    "from ignite.metrics import MeanSquaredError, Loss\n",
    "from ignite.contrib.metrics import AveragePrecision, ROC_AUC\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, crit, optim, writer, train_loader, val_loader, test_loader, modelname):\n",
    "        self.model = model\n",
    "        self.crit = crit\n",
    "        self.optim = optim\n",
    "        self.writer = writer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.best_model = None\n",
    "        self.best_AP = -1.0\n",
    "        self.modelname = modelname\n",
    "        \n",
    "        self.trainer = create_supervised_trainer(model, optim, crit)\n",
    "        self.metrics = {'loss': Loss(crit), 'ap': AveragePrecision(), \"roc\": ROC_AUC()}\n",
    "        self.evaluator = create_supervised_evaluator(model, metrics=self.metrics)\n",
    "        \n",
    "        ## Add events\n",
    "        self.trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=self.log_training_loss)\n",
    "        self.trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=self.log_validation_results)\n",
    "        self.trainer.add_event_handler(event_name=Events.COMPLETED, handler=self.log_test_results)\n",
    "        \n",
    "        print(model)\n",
    "        \n",
    "    def log_training_loss(self, engine, log_interval=400):\n",
    "        epoch = engine.state.epoch\n",
    "        itr = engine.state.iteration\n",
    "        fmt = \"TRAIN: Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "        msg = fmt.format(epoch, itr, len(self.train_loader), engine.state.output)\n",
    "        self.model.itr = itr\n",
    "        if itr % log_interval == 0:\n",
    "            print(msg)\n",
    "#             self.evaluator.run(self.train_loader)\n",
    "            \n",
    "#             metrics = self.evaluator.state.metrics\n",
    "#             mse = metrics['loss']\n",
    "#             ap = metrics['ap']\n",
    "#             roc = metrics['roc']\n",
    "            \n",
    "#             print(\"Epoch[{}] Validation MSE: {:.2f} Avg Prec: {:.2f} ROC: {:.2f} \"\n",
    "#                   .format(engine.state.epoch, mse, ap, roc))\n",
    "            \n",
    "#             self.writer.add_scalar(\"training/mse\", mse, engine.state.epoch)\n",
    "#             self.writer.add_scalar(\"training/ap\", ap, engine.state.epoch)\n",
    "#             self.writer.add_scalar(\"training/roc\", roc, engine.state.epoch)\n",
    "            \n",
    "    def log_validation_results(self, engine):\n",
    "        self.evaluator.run(self.val_loader)\n",
    "        \n",
    "        metrics = self.evaluator.state.metrics\n",
    "        mse = metrics['loss']\n",
    "        ap = metrics['ap']\n",
    "        roc = metrics['roc']\n",
    "        \n",
    "        if ap > self.best_AP:\n",
    "            self.best_AP = ap\n",
    "            self.best_model = model.state_dict()\n",
    "        \n",
    "        print(\"VALIDATION Epoch[{}] Validation MSE: {:.2f} Avg Prec: {:.2f} ROC: {:.2f} \"\n",
    "              .format(engine.state.epoch, mse, ap, roc))\n",
    "        self.writer.add_scalar(\"validation/mse\", mse, engine.state.epoch)\n",
    "        self.writer.add_scalar(\"validation/ap\", ap, engine.state.epoch)\n",
    "        self.writer.add_scalar(\"validation/roc\", roc, engine.state.epoch)\n",
    "\n",
    "    def log_test_results(self, engine):\n",
    "        self.evaluator.run(self.test_loader)\n",
    "        \n",
    "        metrics = self.evaluator.state.metrics\n",
    "        mse = metrics['loss']\n",
    "        ap = metrics['ap']\n",
    "        roc = metrics['roc']\n",
    "\n",
    "        print(\"TEST: Epoch[{}] Validation MSE: {:.2f} Avg Prec: {:.2f} ROC: {:.2f}\"\n",
    "              .format(engine.state.epoch, mse, ap, roc))\n",
    "        \n",
    "        print(\"BEST AP: \", self.best_AP)\n",
    "        torch.save(self.best_model, './{}.pt'.format(self.modelname))\n",
    "        \n",
    "    def run(self, epochs):\n",
    "        self.trainer.run(self.train_loader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "k =2793\n",
    "# regularizing bias\n",
    "c_bias = 1e-4\n",
    "c_vector = 1e-4\n",
    "batchsize = bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/FLU_simple_mf_02_bias_2019-05-20_23:26:02.195029\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'runs/FLU_simple_mf_02_bias_' + str(datetime.now()).replace(' ', '_')\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "model = BMF(len(vs_map), len(hs_map), vfeats, hfeats, writer=writer, c_bias=c_bias, c_vector=c_vector)\n",
    "crit = model.loss\n",
    "\n",
    "model.cuda()\n",
    "optim = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMF(\n",
      "  (virus): Embedding(203, 2793)\n",
      "  (human): Embedding(2727, 2793)\n",
      "  (bias_virus): Embedding(203, 1)\n",
      "  (bias_human): Embedding(2727, 1)\n",
      "  (vfeats): Embedding(203, 2793)\n",
      "  (hfeats): Embedding(2727, 2793)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "flutrainer = Trainer(model, crit, optim, writer, flutrain_loader, fluval_loader, flutrain_loader, 'flumodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch[1] Iteration[400/3114] Loss: 36458681335808.00\n"
     ]
    }
   ],
   "source": [
    "flutrainer.run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python deeplearning",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
