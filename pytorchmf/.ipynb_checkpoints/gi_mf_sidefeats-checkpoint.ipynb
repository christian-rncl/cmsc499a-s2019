{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "bs = 64\n",
    "test_pct = .10\n",
    "val_pct = .20\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "from make_matrix import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, Sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gidf = pd.read_csv('gi_dset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>effector</th>\n",
       "      <th>zscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AME1</td>\n",
       "      <td>BME0304</td>\n",
       "      <td>-2.416447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AME1</td>\n",
       "      <td>BME0390</td>\n",
       "      <td>-3.338498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AME1</td>\n",
       "      <td>BME0736</td>\n",
       "      <td>-2.054660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AME1</td>\n",
       "      <td>BME1013</td>\n",
       "      <td>-4.013577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AME1</td>\n",
       "      <td>BME1044</td>\n",
       "      <td>-2.855403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gene effector    zscore\n",
       "0  AME1  BME0304 -2.416447\n",
       "1  AME1  BME0390 -3.338498\n",
       "2  AME1  BME0736 -2.054660\n",
       "3  AME1  BME1013 -4.013577\n",
       "4  AME1  BME1044 -2.855403"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create conjoint features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = gidf['gene'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genes.txt', 'w') as f:  # Use file to refer to the file object\n",
    "\n",
    "    f.write(' '.join(genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "from Bio.Alphabet import ProteinAlphabet\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pickle.load(open('D.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dict.fromkeys(['A', 'G','V'], 1)\n",
    "classes.update(dict.fromkeys(['I', 'L', 'F', 'P'], 2))\n",
    "classes.update(dict.fromkeys(['Y', 'M', 'T', 'S'], 3))\n",
    "classes.update(dict.fromkeys(['H', 'N', 'Q', 'W'], 4))\n",
    "classes.update(dict.fromkeys(['R', 'K'], 5))\n",
    "classes.update(dict.fromkeys(['D', 'E'], 6))\n",
    "classes.update(dict.fromkeys(['C', 'U'], 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protToClass(p):\n",
    "    if p == 'X':\n",
    "        return '-1'\n",
    "    else:\n",
    "        return str(classes[p])\n",
    "\n",
    "def seqToClass(seq):\n",
    "    return ''.join(list(map(protToClass, seq)))\n",
    "\n",
    "def normalize(Fi):\n",
    "    return (Fi - min(Fi)) / max(Fi)\n",
    "\n",
    "def getFi(D, seq):\n",
    "    grptoi = {p:i for i,p in enumerate(D)} # group to index mappings\n",
    "    Fi = np.zeros(len(grptoi.values()))\n",
    "    \n",
    "    classSeq = seqToClass(seq)\n",
    "    \n",
    "    for p in D:\n",
    "        Fi[grptoi[p]] += classSeq.count(''.join(p))\n",
    "\n",
    "    return normalize(Fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proteinize(records):\n",
    "    for r in records:\n",
    "        r.seq.Alphabet = ProteinAlphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeF(records):\n",
    "    n = len(records)\n",
    "    \n",
    "    F = np.zeros((n, 2793 + 1))\n",
    "    uniprottoi = {}\n",
    "    itouniprot = {}\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "        F[i,:] = np.append(getFi(D, records[i]), [1]) # add bias\n",
    "        virus_name = records[i].name.split('|')[1]\n",
    "        uniprottoi[virus_name] = i\n",
    "        itouniprot[i] = virus_name\n",
    "        \n",
    "    return F, uniprottoi, itouniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots = list(SeqIO.parse('uniprot-yourlist_M201905206746803381A1F0E0DB47453E0216320D145905I.fasta', 'fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinize(prots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqRecord(seq=Seq('MDRDTKLAFRLRGSHSRRTDDIDDDVIVFKTPNAVYREENSPIQSPVQPILSSP...PSL', SingleLetterAlphabet()), id='sp|P38313|CENPU_YEAST', name='sp|P38313|CENPU_YEAST', description='sp|P38313|CENPU_YEAST Inner kinetochore subunit AME1 OS=Saccharomyces cerevisiae (strain ATCC 204508 / S288c) OX=559292 GN=AME1 PE=1 SV=1', dbxrefs=[])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGene(fasta):\n",
    "    # fasta desc looks like: sp|P38313|CENPU_YEAS .... GN=AME1 PE=1 SV=1\n",
    "    # interested in the word after GN=\n",
    "    match = re.search('GN=(\\w+)', fasta.description)\n",
    "    return match.group().split('=')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary with k:v gene:Fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidefeats_map = {}\n",
    "\n",
    "for fasta in prots:\n",
    "    gene = getGene(fasta)\n",
    "    sidefeats_map[gene] = getFi(D, fasta.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add manually \n",
    "sidefeats_map['HHF2'] = sidefeats_map['HHF1']\n",
    "sidefeats_map['YFL013W'] = sidefeats_map['YFL013W-A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sidefeats_map.pkl', 'wb') as p:\n",
    "    pkl.dump(sidefeats_map, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('sidefeats_map.pkl', 'rb') as p:\n",
    "    # test\n",
    "    smap = pkl.load(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to make a numpy array of side features so we can easily load them as embeddings with no gradient allowing for minibatch lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros((len(gidf.gene.unique()), len(smap['AME1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AME1', 'STU1', 'DAM1', ..., 'SEC18', 'DIM1', 'CDC25'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig = pd.read_csv('gi_dset.csv')\n",
    "orig['gene'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'YFL013W'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-ff8de282affb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgene\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gene'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgs_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgene\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msidefeats_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgene\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'YFL013W'"
     ]
    }
   ],
   "source": [
    "for gene in orig['gene'].unique():\n",
    "    arr[gs_map[gene],:] = sidefeats_map[gene]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalize proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(df):\n",
    "    genes = df['gene'].unique()\n",
    "    effectors = df['effector'].unique()\n",
    "    gs_map = {v:i for i,v in enumerate(genes)}\n",
    "    es_map = {h:i for i,h in enumerate(effectors)}\n",
    "    \n",
    "    df.gene = df.gene.apply(lambda x : gs_map[x])\n",
    "    df.effector = df.effector.apply(lambda x : es_map[x])\n",
    "    df = df.sample(frac=1).reset_index()\n",
    "    return df[['gene', 'effector', 'zscore']], gs_map, es_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gidf, gs_map, es_map = numericalize(gidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>effector</th>\n",
       "      <th>zscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>22</td>\n",
       "      <td>1.110707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3202</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.362778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>729</td>\n",
       "      <td>16</td>\n",
       "      <td>1.790410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2559</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.972562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4176</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.707976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gene  effector    zscore\n",
       "0    85        22  1.110707\n",
       "1  3202        13 -0.362778\n",
       "2   729        16  1.790410\n",
       "3  2559         5 -0.972562\n",
       "4  4176        16 -0.707976"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_to_torch(arr, dtype):\n",
    "    t =  torch.from_numpy(np.array(arr)).type(dtype).to(device)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoaders(df):\n",
    "    X = list(zip(df.gene.values, df.effector.values))\n",
    "    y = df['zscore'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_pct, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_pct, random_state=1)\n",
    "    \n",
    "    train_dset = TensorDataset(arr_to_torch(X_train, torch.long), arr_to_torch(y_train, torch.float32))\n",
    "    train_loader = DataLoader(train_dset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    val_dset = TensorDataset(arr_to_torch(X_val, torch.long), arr_to_torch(y_val, torch.float32))\n",
    "    val_loader = DataLoader(val_dset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    test_dset = TensorDataset(arr_to_torch(X_test, torch.long), arr_to_torch(y_test, torch.float32))\n",
    "    test_loader = DataLoader(test_dset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = getLoaders(gidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_virus, n_human, k=2793, c_vector=1.0, c_bias=1.0, writer=None):\n",
    "        super(MF, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = k\n",
    "        self.n_virus = n_virus\n",
    "        self.n_human = n_human\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.virus = nn.Embedding(n_virus, k)\n",
    "        self.human = nn.Embedding(n_human, k)\n",
    "        \n",
    "        self.bias_virus = nn.Embedding(n_virus, 1)\n",
    "        self.bias_human = nn.Embedding(n_human, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        virus_id = train_x[:, 0]\n",
    "        human_id = train_x[:, 1]\n",
    "        vector_virus = self.virus(virus_id)\n",
    "        vector_human = self.human(human_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_virus = self.bias_virus(virus_id).squeeze()\n",
    "        bias_human = self.bias_human(human_id).squeeze()\n",
    "        biases = (self.bias + bias_virus + bias_human)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_virus * vector_human, dim=1)\n",
    "        \n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "#         loss_mse = F.binary_cross_entropy_with_logits(prediction, target.squeeze())\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "    \n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_virus =  l2_regularize(self.bias_virus.weight) * self.c_bias\n",
    "        prior_bias_human = l2_regularize(self.bias_human.weight) * self.c_bias\n",
    "        \n",
    "        prior_virus =  l2_regularize(self.virus.weight.data) * self.c_vector\n",
    "        prior_human = l2_regularize(self.human.weight.data) * self.c_vector\n",
    "        total = loss_mse + prior_virus + prior_human + prior_bias_virus + prior_bias_human\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss, Accuracy, Precision, Recall\n",
    "from tensorboardX import SummaryWriter\n",
    "from ignite.metrics import MeanSquaredError, Loss\n",
    "from ignite.contrib.metrics import AveragePrecision, ROC_AUC\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, crit, optim, writer, train_loader, val_loader, test_loader, modelname):\n",
    "        self.model = model\n",
    "        self.crit = crit\n",
    "        self.optim = optim\n",
    "        self.writer = writer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.best_model = None\n",
    "        self.best_acc = 999\n",
    "        self.modelname = modelname\n",
    "        \n",
    "        self.trainer = create_supervised_trainer(model, optim, crit)\n",
    "        self.metrics = {'loss': Loss(crit), \"acc\": MeanSquaredError()}\n",
    "        self.evaluator = create_supervised_evaluator(model, metrics=self.metrics)\n",
    "        \n",
    "        ## Add events\n",
    "        self.trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=self.log_training_loss)\n",
    "        self.trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=self.log_validation_results)\n",
    "        self.trainer.add_event_handler(event_name=Events.COMPLETED, handler=self.log_test_results)\n",
    "        \n",
    "        print(model)\n",
    "        \n",
    "    def log_training_loss(self, engine, log_interval=400):\n",
    "        epoch = engine.state.epoch\n",
    "        itr = engine.state.iteration\n",
    "        fmt = \"TRAIN: Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "        msg = fmt.format(epoch, itr, len(self.train_loader), engine.state.output)\n",
    "        self.model.itr = itr\n",
    "        if itr % log_interval == 0:\n",
    "            print(msg)\n",
    "#             self.evaluator.run(self.train_loader)\n",
    "            \n",
    "#             metrics = self.evaluator.state.metrics\n",
    "#             mse = metrics['loss']\n",
    "#             ap = metrics['ap']\n",
    "#             roc = metrics['roc']\n",
    "            \n",
    "#             print(\"Epoch[{}] Validation MSE: {:.2f} Avg Prec: {:.2f} ROC: {:.2f} \"\n",
    "#                   .format(engine.state.epoch, mse, ap, roc))\n",
    "            \n",
    "#             self.writer.add_scalar(\"training/mse\", mse, engine.state.epoch)\n",
    "#             self.writer.add_scalar(\"training/ap\", ap, engine.state.epoch)\n",
    "#             self.writer.add_scalar(\"training/roc\", roc, engine.state.epoch)\n",
    "            \n",
    "    def log_validation_results(self, engine):\n",
    "        self.evaluator.run(self.val_loader)\n",
    "        \n",
    "        metrics = self.evaluator.state.metrics\n",
    "        mse = metrics['loss']\n",
    "        acc = metrics['acc']\n",
    "        \n",
    "        if acc < self.best_acc:\n",
    "            self.best_acc = acc\n",
    "            self.best_model = model.state_dict()\n",
    "        \n",
    "        print(\"VALIDATION Epoch[{}] Validation MSE: {:.2f} Acc: {:.2f}\"\n",
    "              .format(engine.state.epoch, mse, acc))\n",
    "        self.writer.add_scalar(\"validation/mse\", mse, engine.state.epoch)\n",
    "        self.writer.add_scalar(\"validation/acc\", acc, engine.state.epoch)\n",
    "        \n",
    "    def log_test_results(self, engine):\n",
    "        self.evaluator.run(self.test_loader)\n",
    "        \n",
    "        metrics = self.evaluator.state.metrics\n",
    "        mse = metrics['loss']\n",
    "        acc = metrics['acc']\n",
    "\n",
    "\n",
    "        print(\"TEST: Epoch[{}] Validation MSE: {:.2f} Acc: {:.2f}\"\n",
    "              .format(engine.state.epoch, mse, acc))\n",
    "        \n",
    "        print(\"BEST Acc: \", self.best_acc)\n",
    "        torch.save(self.best_model, './{}.pt'.format(self.modelname))\n",
    "        \n",
    "    def run(self, epochs):\n",
    "        self.trainer.run(self.train_loader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "k =20\n",
    "# regularizing bias\n",
    "c_bias = 1e-4\n",
    "c_vector = 1e-4\n",
    "batchsize = bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/FLU_simple_mf_02_bias_2019-05-19_23:34:59.604969\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'runs/FLU_simple_mf_02_bias_' + str(datetime.now()).replace(' ', '_')\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "model = MF(len(gs_map), len(es_map), writer=writer, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "crit = model.loss\n",
    "model.cuda()\n",
    "optim = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF(\n",
      "  (virus): Embedding(4450, 20)\n",
      "  (human): Embedding(36, 20)\n",
      "  (bias_virus): Embedding(4450, 1)\n",
      "  (bias_human): Embedding(36, 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, crit, optim, writer, train_loader, val_loader, test_loader, 'flumodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch[1] Iteration[400/1897] Loss: 28.52\n",
      "TRAIN: Epoch[1] Iteration[800/1897] Loss: 23.36\n",
      "TRAIN: Epoch[1] Iteration[1200/1897] Loss: 20.53\n",
      "TRAIN: Epoch[1] Iteration[1600/1897] Loss: 21.19\n",
      "VALIDATION Epoch[1] Validation MSE: 19.49 Acc: 10.70\n",
      "TRAIN: Epoch[2] Iteration[2000/1897] Loss: 17.09\n",
      "TRAIN: Epoch[2] Iteration[2400/1897] Loss: 13.82\n",
      "TRAIN: Epoch[2] Iteration[2800/1897] Loss: 14.97\n",
      "TRAIN: Epoch[2] Iteration[3200/1897] Loss: 15.68\n",
      "TRAIN: Epoch[2] Iteration[3600/1897] Loss: 14.66\n",
      "VALIDATION Epoch[2] Validation MSE: 13.91 Acc: 5.49\n",
      "TEST: Epoch[2] Validation MSE: 13.87 Acc: 5.46\n",
      "BEST Acc:  5.493472703468645\n"
     ]
    }
   ],
   "source": [
    "trainer.run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python deeplearning",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
